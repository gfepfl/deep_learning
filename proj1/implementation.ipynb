{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from dlc_practical_prologue import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 0 if first > second else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target, test_input, test_target = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = generate_pair_sets(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10bb410f0>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_hidden = 20\n",
    "output_size = 1\n",
    "mini_batch_size = 100\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Flatten()\n",
      "  (5): Linear(in_features=2000, out_features=20, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "simple_model = nn.Sequential(\n",
    "    nn.Conv2d(2,10,3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(10,20,3),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(10*10*20, nb_hidden),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nb_hidden, output_size)\n",
    ")\n",
    "print(simple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_input, train_target, mini_batch_size):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "    eta = 1e-1\n",
    "\n",
    "    for e in range(25):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= eta * p.grad\n",
    "        print(e, sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.06580162633767e+32\n",
      "1 1.3208230116981675e+31\n",
      "2 1.3099506361844967e+29\n",
      "3 1.29916452057638e+27\n",
      "4 1.2884719414668735e+25\n",
      "5 1.2778636932072714e+23\n",
      "6 1.2673461304976118e+21\n",
      "7 1.2569107992290525e+19\n",
      "8 1.2465658090907238e+17\n",
      "9 1236300757729280.0\n",
      "10 12261244567552.0\n",
      "11 121603032448.0\n",
      "12 1206021260.5\n",
      "13 11960891.65625\n",
      "14 118625.87664794922\n",
      "15 1178.8254685401917\n",
      "16 14.124420404434204\n",
      "17 2.583320900797844\n",
      "18 2.469860404729843\n",
      "19 2.468830868601799\n",
      "20 2.4688309878110886\n",
      "21 2.4688312113285065\n",
      "22 2.4688340425491333\n",
      "23 2.468834951519966\n",
      "24 2.468834713101387\n",
      "0 2.468834161758423\n",
      "1 2.468834161758423\n",
      "2 2.468834161758423\n",
      "3 2.468834161758423\n",
      "4 2.468834161758423\n",
      "5 2.468834161758423\n",
      "6 2.468834161758423\n",
      "7 2.468834161758423\n",
      "8 2.468834161758423\n",
      "9 2.468834161758423\n",
      "10 2.468834161758423\n",
      "11 2.468834161758423\n",
      "12 2.468834161758423\n",
      "13 2.468834161758423\n",
      "14 2.468834161758423\n",
      "15 2.468834161758423\n",
      "16 2.468834161758423\n",
      "17 2.468834161758423\n",
      "18 2.468834161758423\n",
      "19 2.468834161758423\n",
      "20 2.468834161758423\n",
      "21 2.468834161758423\n",
      "22 2.468834161758423\n",
      "23 2.468834161758423\n",
      "24 2.468834161758423\n"
     ]
    }
   ],
   "source": [
    "for k in range(2):\n",
    "    train_model(simple_model, train_input, train_target.float(), mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  1538.4516179668904\n",
      "Training loss:  0.0282022967338562\n",
      "Training loss:  0.025283517837524413\n",
      "Training loss:  0.02269571805000305\n",
      "Training loss:  0.02040138852596283\n",
      "Training loss:  0.01836720597743988\n",
      "Training loss:  0.016563721656799316\n",
      "Training loss:  0.014964737415313721\n",
      "Training loss:  0.01354707396030426\n",
      "Training loss:  0.01229017722606659\n",
      "Training loss:  0.011175804615020752\n",
      "Training loss:  0.010187793612480164\n",
      "Training loss:  0.009311840891838074\n",
      "Training loss:  0.008535197913646698\n",
      "Training loss:  0.007846644937992095\n",
      "Training loss:  0.007236156344413758\n",
      "Training loss:  0.006694900929927826\n",
      "Training loss:  0.006215029239654541\n",
      "Training loss:  0.005789560109376908\n",
      "Training loss:  0.005412348240613938\n",
      "Training loss:  0.005077907472848892\n",
      "Training loss:  0.0047813884317874904\n",
      "Training loss:  0.00451849702000618\n",
      "Training loss:  0.004285416096448899\n",
      "Training loss:  0.004078767448663712\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for b in range(0, train_input.size(0), mini_batch_size):\n",
    "        \n",
    "        # setting gradient to zeros\n",
    "        optimizer.zero_grad()        \n",
    "        output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "        loss = criterion(output, train_target.narrow(0, b, mini_batch_size).float())\n",
    "        \n",
    "        # backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the gradient to new gradients\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Training loss: \",(running_loss/len(train_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
